#!/usr/bin/env python3
"""
=============================================================================
CATALOG GENERATOR FOR SUBSCRIPTION-LEVEL AGGREGATION
=============================================================================

This script generates a catalog folder that aggregates all RG-level outputs
into a single subscription-level state.

Usage:
    python generate_catalog.py /path/to/subscription/folder [--config resource_types.yaml]

Example:
    python generate_catalog.py /infrastructure-repo/legacy-import/sub-prod-core

Input Structure:
    /sub-prod-core/
    ├── /rg-hub-network/
    │   ├── main.tf
    │   ├── locals.tf
    │   ├── outputs.tf
    │   └── providers.tf
    ├── /rg-shared-services/
    │   └── ...
    └── /rg-identity/
        └── ...

Output Structure:
    /sub-prod-core/
    ├── /rg-hub-network/
    ├── /rg-shared-services/
    ├── /rg-identity/
    └── /catalog/              ◄── GENERATED
        ├── data.tf            ◄── terraform_remote_state for each RG
        ├── outputs.tf         ◄── Merged outputs by resource type
        └── providers.tf       ◄── Backend configuration

=============================================================================
"""

import argparse
import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Set, Optional
from dataclasses import dataclass

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

try:
    from config_loader import get_config, get_backend_config, get_provider_version, get_catalog_output_keys
    HAS_CONFIG_LOADER = True
except ImportError:
    HAS_CONFIG_LOADER = False


# =============================================================================
# DEFAULT CONFIGURATION
# =============================================================================

DEFAULT_OUTPUT_KEYS = [
    "vnets",
    "subnets",
    "nsgs",
    "nics",
    "public_ips",
    "route_tables",
    "nat_gateways",
    "linux_vms",
    "windows_vms",
    "classic_vms",
    "managed_disks",
    "availability_sets",
    "vmss",
    "storage_accounts",
    "storage_containers",
    "sql_servers",
    "sql_databases",
    "cosmosdb_accounts",
    "key_vaults",
    "managed_identities",
    "resource_groups",
]

DEFAULT_BACKEND_CONFIG = {
    "resource_group_name": "tfstate-rg",
    "storage_account_name": "tfstatestore",
    "container_name": "tfstate",
}


def get_default_backend_config() -> Dict[str, str]:
    """Get backend configuration from config loader or defaults."""
    if HAS_CONFIG_LOADER:
        return get_backend_config()
    return DEFAULT_BACKEND_CONFIG


def get_default_output_keys() -> List[str]:
    """Get output keys from config loader or defaults."""
    if HAS_CONFIG_LOADER:
        return get_catalog_output_keys()
    return DEFAULT_OUTPUT_KEYS


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def find_rg_folders(subscription_path: Path) -> List[Path]:
    """Find all RG folders in a subscription folder."""
    rg_folders = []
    
    for item in subscription_path.iterdir():
        if item.is_dir() and item.name != "catalog":
            # Check if it looks like an RG folder (has main.tf or outputs.tf)
            if (item / "main.tf").exists() or (item / "outputs.tf").exists():
                rg_folders.append(item)
    
    return sorted(rg_folders)


def extract_outputs_from_tf(outputs_tf_path: Path) -> Set[str]:
    """Extract output names from an outputs.tf file."""
    if not outputs_tf_path.exists():
        return set()
    
    content = outputs_tf_path.read_text()
    # Match output "name" { patterns
    pattern = re.compile(r'output\s+"([^"]+)"\s*\{')
    matches = pattern.findall(content)
    
    # Filter out metadata outputs
    return {m for m in matches if not m.startswith("_")}


def extract_backend_key(providers_tf_path: Path) -> Optional[str]:
    """Extract the backend key from providers.tf."""
    if not providers_tf_path.exists():
        return None
    
    content = providers_tf_path.read_text()
    pattern = re.compile(r'key\s*=\s*"([^"]+)"')
    match = pattern.search(content)
    
    return match.group(1) if match else None


# =============================================================================
# GENERATORS
# =============================================================================

def generate_data_tf(rg_folders: List[Path], subscription_name: str, 
                     backend_config: Dict[str, str]) -> str:
    """Generate data.tf with terraform_remote_state for each RG."""
    lines = [
        "# =============================================================================",
        "# DATA SOURCES - Auto-generated by generate_catalog.py",
        "# =============================================================================",
        "# This file reads the state from each RG in this subscription.",
        "# DO NOT EDIT MANUALLY - regenerate using the script if RGs change.",
        "# =============================================================================",
        "",
    ]
    
    for rg_folder in rg_folders:
        rg_name = rg_folder.name
        # Convert folder name to a valid Terraform identifier
        tf_name = rg_name.replace("-", "_")
        
        # Try to get the actual backend key from providers.tf
        providers_path = rg_folder / "providers.tf"
        backend_key = extract_backend_key(providers_path)
        
        if not backend_key:
            # Generate a default key based on convention
            backend_key = f"legacy/{subscription_name}/{rg_name}.tfstate"
        
        lines.extend([
            f'data "terraform_remote_state" "{tf_name}" {{',
            '  backend = "azurerm"',
            '  config = {',
            f'    resource_group_name  = "{backend_config["resource_group_name"]}"',
            f'    storage_account_name = "{backend_config["storage_account_name"]}"',
            f'    container_name       = "{backend_config["container_name"]}"',
            f'    key                  = "{backend_key}"',
            '  }',
            '}',
            '',
        ])
    
    return "\n".join(lines)


def generate_outputs_tf(rg_folders: List[Path], output_keys: List[str]) -> str:
    """Generate outputs.tf that merges all RG outputs."""
    lines = [
        "# =============================================================================",
        "# OUTPUTS - Auto-generated by generate_catalog.py", 
        "# =============================================================================",
        "# This file merges outputs from all RGs into unified maps by resource type.",
        "# Greenfield code reads this catalog for a single source of truth.",
        "# DO NOT EDIT MANUALLY - regenerate using the script if RGs change.",
        "# =============================================================================",
        "",
    ]
    
    # Build list of terraform_remote_state references
    state_refs = []
    for rg_folder in rg_folders:
        tf_name = rg_folder.name.replace("-", "_")
        state_refs.append(tf_name)
    
    # For each output key, merge across all RGs
    for output_key in output_keys:
        lines.append(f'output "all_{output_key}" {{')
        lines.append(f'  description = "All {output_key} from all RGs in this subscription"')
        lines.append('  value = merge(')
        
        for tf_name in state_refs:
            # Use try() to handle RGs that don't have this output type
            lines.append(f'    try(data.terraform_remote_state.{tf_name}.outputs.{output_key}, {{}}),')
        
        lines.append('  )')
        lines.append('}')
        lines.append('')
    
    # Add a "by_rg" output for granular access
    lines.extend([
        '# Access resources by RG if needed',
        'output "by_rg" {',
        '  description = "All outputs organized by source RG"',
        '  value = {',
    ])
    
    for tf_name in state_refs:
        # Convert back to readable name
        rg_name = tf_name.replace("_", "-")
        lines.append(f'    "{rg_name}" = data.terraform_remote_state.{tf_name}.outputs')
    
    lines.extend([
        '  }',
        '}',
        '',
    ])
    
    # Add metadata
    rg_names_list = ', '.join([f'"{f.name}"' for f in rg_folders])
    lines.extend([
        '# Catalog metadata',
        'output "_catalog_metadata" {',
        '  description = "Metadata about this catalog"',
        '  value = {',
        f'    rg_count       = {len(rg_folders)}',
        f'    rg_names       = [{rg_names_list}]',
        '    generated_by   = "generate_catalog.py"',
        '  }',
        '}',
        '',
    ])
    
    return "\n".join(lines)


def generate_providers_tf(subscription_name: str, backend_config: Dict[str, str]) -> str:
    """Generate providers.tf for the catalog."""
    # Get versions from config
    if HAS_CONFIG_LOADER:
        tf_version = get_config().get('terraform', {}).get('required_version', '>= 1.5.0')
        azurerm_version = get_provider_version('azurerm')
    else:
        tf_version = ">= 1.5.0"
        azurerm_version = "~> 4.0"

    return f'''# =============================================================================
# PROVIDERS - Catalog for {subscription_name}
# =============================================================================

terraform {{
  required_version = "{tf_version}"

  required_providers {{
    azurerm = {{
      source  = "hashicorp/azurerm"
      version = "{azurerm_version}"
    }}
  }}

  backend "azurerm" {{
    resource_group_name  = "{backend_config["resource_group_name"]}"
    storage_account_name = "{backend_config["storage_account_name"]}"
    container_name       = "{backend_config["container_name"]}"
    key                  = "legacy/{subscription_name}/catalog.tfstate"
  }}
}}

# Note: No provider block needed - catalog only uses data sources
'''


def generate_readme(subscription_name: str, rg_folders: List[Path]) -> str:
    """Generate a README for the catalog folder."""
    rg_list = "\n".join([f"  - {f.name}" for f in rg_folders])
    
    return f'''# Catalog for {subscription_name}

This catalog aggregates all legacy RG outputs in the `{subscription_name}` subscription.

## Source RGs

{rg_list}

## Usage in Greenfield

```hcl
# In your modern-platform code:

data "terraform_remote_state" "legacy" {{
  backend = "azurerm"
  config = {{
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstatestore"
    container_name       = "tfstate"
    key                  = "legacy/{subscription_name}/catalog.tfstate"
  }}
}}

# Access any resource by Azure name:
locals {{
  hub_vnet_id     = data.terraform_remote_state.legacy.outputs.all_vnets["hub-vnet"].id
  app_subnet_id   = data.terraform_remote_state.legacy.outputs.all_subnets["app-vnet/app-subnet"].id
  web_vm_ip       = data.terraform_remote_state.legacy.outputs.all_linux_vms["web-server-01"].private_ip
}}
```

## Regenerating

If RGs are added or removed, regenerate this catalog:

```bash
python generate_catalog.py /path/to/{subscription_name}
```

## Available Outputs

- `all_vnets` - All Virtual Networks
- `all_subnets` - All Subnets
- `all_nsgs` - All Network Security Groups
- `all_linux_vms` - All Linux VMs
- `all_windows_vms` - All Windows VMs
- `all_managed_disks` - All Managed Disks
- `all_storage_accounts` - All Storage Accounts
- `all_key_vaults` - All Key Vaults
- `by_rg` - All outputs organized by source RG

'''


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Generate catalog folder for subscription-level aggregation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python generate_catalog.py ./sub-prod-core
    python generate_catalog.py /infra/legacy-import/sub-prod-core --backend-rg my-tfstate-rg

The script will create a /catalog/ folder with data.tf, outputs.tf, and providers.tf.
        """
    )
    
    parser.add_argument(
        "subscription_folder",
        help="Path to the subscription folder containing RG subfolders"
    )
    
    # Get defaults from config
    default_backend = get_default_backend_config()

    parser.add_argument(
        "--backend-rg",
        help="Resource group name for tfstate backend",
        default=default_backend.get("resource_group_name", "tfstate-rg")
    )

    parser.add_argument(
        "--backend-storage",
        help="Storage account name for tfstate backend",
        default=default_backend.get("storage_account_name", "tfstatestore")
    )

    parser.add_argument(
        "--backend-container",
        help="Container name for tfstate backend",
        default=default_backend.get("container_name", "tfstate")
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="Print output to stdout instead of writing files"
    )
    
    parser.add_argument(
        "--output-keys",
        help="Comma-separated list of output keys to include",
        default=None
    )
    
    args = parser.parse_args()
    
    # Setup paths
    subscription_path = Path(args.subscription_folder)
    if not subscription_path.exists():
        print(f"Error: Subscription folder not found: {subscription_path}")
        sys.exit(1)
    
    subscription_name = subscription_path.name
    catalog_path = subscription_path / "catalog"
    
    # Build backend config
    backend_config = {
        "resource_group_name": args.backend_rg,
        "storage_account_name": args.backend_storage,
        "container_name": args.backend_container,
    }
    
    # Find RG folders
    print(f"Scanning {subscription_path} for RG folders...")
    rg_folders = find_rg_folders(subscription_path)
    
    if not rg_folders:
        print("Error: No RG folders found")
        sys.exit(1)
    
    print(f"Found {len(rg_folders)} RG folders:")
    for f in rg_folders:
        print(f"  - {f.name}")
    
    # Determine output keys
    if args.output_keys:
        output_keys = args.output_keys.split(",")
    else:
        output_keys = get_default_output_keys()
    
    # Generate files
    data_tf = generate_data_tf(rg_folders, subscription_name, backend_config)
    outputs_tf = generate_outputs_tf(rg_folders, output_keys)
    providers_tf = generate_providers_tf(subscription_name, backend_config)
    readme = generate_readme(subscription_name, rg_folders)
    
    if args.dry_run:
        print("\n" + "=" * 60)
        print("DATA.TF")
        print("=" * 60)
        print(data_tf)
        print("\n" + "=" * 60)
        print("OUTPUTS.TF")
        print("=" * 60)
        print(outputs_tf)
        print("\n" + "=" * 60)
        print("PROVIDERS.TF")
        print("=" * 60)
        print(providers_tf)
    else:
        # Create catalog folder
        catalog_path.mkdir(exist_ok=True)
        
        # Write files
        (catalog_path / "data.tf").write_text(data_tf)
        (catalog_path / "outputs.tf").write_text(outputs_tf)
        (catalog_path / "providers.tf").write_text(providers_tf)
        (catalog_path / "README.md").write_text(readme)
        
        print(f"\nGenerated catalog at: {catalog_path}")
        print("  - data.tf")
        print("  - outputs.tf")
        print("  - providers.tf")
        print("  - README.md")
        print("\nNext steps:")
        print("  1. Ensure all RG states are migrated to remote backend")
        print("  2. cd catalog && terraform init")
        print("  3. terraform plan (should show no changes to make)")
        print("  4. terraform apply (creates the catalog state)")


if __name__ == "__main__":
    main()
